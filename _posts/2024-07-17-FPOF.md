---
title: "Frequent Patterns Outlier Factor "
categories:
    - Machine Learning
    - Mathematics
    - Statistics
    - Data Science
tags:
    - Outlier Detection
    - Unsupervised Learning
    - Data Analysis
author_profile: false
---

# Interpretable Outlier Detection Using FPOF in Machine Learning

Outlier detection is a fundamental task in machine learning, especially in the realm of unsupervised learning where labels are not available. It involves identifying items in a dataset that deviate significantly from the majority of the data. This technique is crucial in various fields such as fraud detection, sensor data analysis, and scientific discovery.

## Importance of Identifying Outliers

Outliers can indicate anomalies, errors, or significant events within the data. For instance, in accounting records, identifying outliers can help in detecting errors or fraudulent transactions. With vast amounts of transactions, manually inspecting each one is impractical. Instead, focusing on the most unusual records can efficiently pinpoint potential issues.

Similarly, in other domains like credit card transactions, sensor readings, weather measurements, and biological data, detecting outliers can reveal errors, system failures, or important discoveries. In scientific research, the most unusual records often hold the most valuable insights.

## The Need for Interpretability in Outlier Detection

In classification and regression tasks, interpretability is often sacrificed for accuracy, especially with complex models like boosted trees. However, for outlier detection, interpretability is crucial. When an outlier is identified, understanding why it is considered unusual is essential for determining the appropriate response.

Without clear reasons behind an outlier detection, the process can lose its value. For example, in credit card transaction monitoring, detecting unusual transactions is only useful if we can understand why they are flagged as suspicious. This interpretability can sometimes be achieved through post-hoc explanations using techniques from Explainable AI (XAI), such as feature importance and proxy models. However, having inherently interpretable models is often more effective.

## Algorithms for Outlier Detection on Tabular Data

Several algorithms are commonly used for outlier detection in tabular data, including:

- **Isolation Forests**
- **Local Outlier Factor (LOF)**
- **k-Nearest Neighbors (KNN)**
- **One-Class Support Vector Machines (SVMs)**

While these methods can effectively identify outliers, they often lack the ability to explain why specific records are flagged. This is especially challenging in datasets with many features or complex relationships among features.

## Frequent Patterns Outlier Factor (FPOF)

FPOF (Frequent Pattern Outlier Factor) is an algorithm that offers interpretability for outlier detection, particularly with categorical data. Unlike most methods that require numerical encoding of categorical features, FPOF works directly with categorical data. For numeric data, it involves binning into categorical ranges.

### The FPOF Algorithm

FPOF identifies Frequent Item Sets (FISs) in the data. FISs are common values or sets of values that frequently appear together. Most datasets contain many FISs due to inherent associations among features. The core idea of FPOF is that normal records contain more and more frequent FISs than outliers.

Here is a step-by-step process using the SpeedDating dataset from OpenML:

```python
from mlxtend.frequent_patterns import apriori
import pandas as pd
from sklearn.datasets import fetch_openml
import warnings

warnings.filterwarnings(action='ignore', category=DeprecationWarning)

# Fetch the data
data = fetch_openml('SpeedDating', version=1, parser='auto') 
data_df = pd.DataFrame(data.data, columns=data.feature_names)

# Select relevant features
data_df = data_df[['d_pref_o_attractive', 'd_pref_o_sincere',
                   'd_pref_o_intelligence', 'd_pref_o_funny',
                   'd_pref_o_ambitious', 'd_pref_o_shared_interests']] 
data_df = pd.get_dummies(data_df) 

# Convert binary features to boolean
for col_name in data_df.columns:
    data_df[col_name] = data_df[col_name].map({0: False, 1: True})

# Identify frequent item sets
frequent_itemsets = apriori(data_df, min_support=0.3, use_colnames=True) 

# Initialize FPOF scores
data_df['FPOF_Score'] = 0

# Calculate FPOF scores
for fis_idx in frequent_itemsets.index: 
    fis = frequent_itemsets.loc[fis_idx, 'itemsets']
    support = frequent_itemsets.loc[fis_idx, 'support'] 
    col_list = (list(fis))
    cond = True
    for col_name in col_list:
        cond = cond & (data_df[col_name])
    data_df.loc[data_df[cond].index, 'FPOF_Score'] += support   

# Normalize FPOF scores
min_score = data_df['FPOF_Score'].min() 
max_score = data_df['FPOF_Score'].max()
data_df['FPOF_Score'] = [(max_score - x) / (max_score - min_score) 
                         for x in data_df['FPOF_Score']]
```